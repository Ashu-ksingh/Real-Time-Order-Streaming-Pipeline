# main.py â€” Orchestrator for Real-Time Streaming Pipeline
import os
import subprocess
import time
import sys

# âœ… Add project root to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.kafka_config import KAFKA_CONFIG

def run_pipeline():
    print("\nðŸš€ Starting Real-Time Order Streaming Pipeline...\n")
    print(f"ðŸ”¹ Kafka Broker: {KAFKA_CONFIG['bootstrap_servers']}")
    print(f"ðŸ”¹ Kafka Topic: {KAFKA_CONFIG['topic']}\n")

    # âœ… Step 1: Start the Kafka Producer
    print("ðŸ“¦ Launching Kafka Producer (sending live order events)...\n")
    producer_process = subprocess.Popen(
        ["python", "scripts/producer.py"],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True
    )

    # Give the producer some time to generate messages before starting Spark
    time.sleep(10)

    # âœ… Step 2: Start Spark Structured Streaming Consumer
    print("\nâš¡ Launching Spark Structured Streaming Consumer...\n")
    consumer_process = subprocess.Popen(
        ["python", "scripts/spark_consumer.py"],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True
    )

    print("\nâœ… Pipeline is now running:")
    print("   â€¢ Producer â†’ generating live orders")
    print("   â€¢ Consumer â†’ processing stream and writing to PostgreSQL\n")
    print("ðŸ’¡ Press Ctrl + C to stop both processes.\n")

    try:
        producer_process.wait()
        consumer_process.wait()
    except KeyboardInterrupt:
        print("\nðŸ›‘ Stopping both processes...")
        producer_process.terminate()
        consumer_process.terminate()


if __name__ == "__main__":
    run_pipeline()
